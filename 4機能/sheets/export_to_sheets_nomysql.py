



import pandas as pd
import sys
import os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
import pickle
from google_auth_oauthlib.flow import InstalledAppFlow
from googleapiclient.discovery import build
from dotenv import load_dotenv
from gmail.fetch_emails import get_gmail_service, fetch_ses_emails
from parser.gemini_parser import GeminiParser
import logging
from datetime import datetime
import base64
import re
import json



# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()

# ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®ã‚¹ã‚³ãƒ¼ãƒ—
SCOPES = ['https://www.googleapis.com/auth/spreadsheets']


import unicodedata

# ã‚»ãƒ«å†…ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã™ã‚‹é–¢æ•°
def sanitize_text(text):
    if not isinstance(text, str):
        text = str(text)
    # æ­£è¦åŒ–ã—ã¦ä¸å¯è¦–æ–‡å­—ã‚’é™¤å»
    text = unicodedata.normalize('NFKC', text)
    # çµµæ–‡å­—ã‚„åˆ¶å¾¡æ–‡å­—ãªã©ã‚’é™¤å»ï¼ˆå¿…è¦ãªã‚‰ï¼‰
    text = ''.join(c for c in text if unicodedata.category(c)[0] != "C")
    return text


def normalize_email_data(email_data_list):
    if isinstance(email_data_list, (dict, list)):  # è¾æ›¸ã¾ãŸã¯ãƒªã‚¹ãƒˆã®å ´åˆ
        # ã¾ãšJSONæ–‡å­—åˆ—ã«å¤‰æ›
        email_data_list = json.dumps(email_data_list)
    elif not isinstance(email_data_list, str):
        raise ValueError("æä¾›çš„è¾“å…¥æ•°æ®ä¸æ˜¯æœ‰æ•ˆçš„å­—ç¬¦ä¸²ã€‚")
    
    try:
        email_data_json = json.loads(email_data_list)
        return email_data_json
    except json.JSONDecodeError:
        raise ValueError("æä¾›çš„å­—ç¬¦ä¸²æ— æ³•è§£æä¸ºæœ‰æ•ˆçš„ JSONã€‚")


# -------------------------
# Google Sheetsã®èªè¨¼
# -------------------------
def get_gspread_service():
    creds = None
    token_path = 'sheets/token_sheets.pickle'

    if os.path.exists(token_path):
        with open(token_path, 'rb') as token:
            creds = pickle.load(token)

    if not creds or not creds.valid:
        flow = InstalledAppFlow.from_client_secrets_file('config/credentials.json', SCOPES)
        creds = flow.run_local_server(port=0)
        with open(token_path, 'wb') as token:
            pickle.dump(creds, token)

    return build('sheets', 'v4', credentials=creds)

# å˜ä¾¡ã®æ–‡å­—åˆ—ã‚’æ•°å€¤ã«å¤‰æ›ã™ã‚‹é–¢æ•°
def clean_unit_price(price_str):
    # å¦‚æœæ˜¯ Series ç›´æ¥è¿”å› None
    if isinstance(price_str, pd.Series):
        return None
    
    if pd.isna(price_str) or price_str == '':
        return None
        
    if isinstance(price_str, (int, float)):
        return float(price_str)
        
    # å¤„ç†å­—ç¬¦ä¸²
    price_str = str(price_str)
    match = re.search(r'(\d+(?:,\d+)?)(?=å††|$)', price_str)
    if match:
        return float(match.group(1).replace(',', ''))
    match_man = re.search(r'(\d+(?:\.\d+)?)\s*ä¸‡', price_str)
    if match_man:
        return float(match_man.group(1)) * 10000
    return None
# -------------------------
# ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã¸ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
# -------------------------
def export_to_sheet(email_data_list,spreadsheet_id, sheet_name="ã‚·ãƒ¼ãƒˆ1"):
    if not email_data_list:
        print("ğŸ“­ æœ‰åŠ¹ãªãƒ¡ãƒ¼ãƒ«ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return
    service = get_gspread_service()
    if isinstance(email_data_list, list):
        df = pd.DataFrame(email_data_list)
    else:
        # ãã‚Œä»¥å¤–ã®å‹ã«å¯¾å¿œã™ã‚‹å‡¦ç†
        df = email_data_list  # æ—¢ã«DataFrameã®å ´åˆ
    if df.empty:
        print("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚’ä¸­æ­¢ã—ã¾ã™ã€‚")
        return
    
    # å°†åˆ—è¡¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼ˆé’ˆå¯¹æŠ€èƒ½åˆ—ï¼‰
    for col in ['required_skills', 'optional_skills']:
        if col in df.columns:
            df[col] = df[col].apply(lambda x: ', '.join(x) if isinstance(x, list) else x)

    if 'unit_price' in df.columns:
        # æå–"ä¸‡"å•ä½çš„æ•°å€¼
        man_yen = df['unit_price'].str.extract(r'(\d+(?:\.\d+)?)\s*ä¸‡').dropna()
        if not man_yen.empty:
            df.loc[man_yen.index, 'unit_price_numeric'] = man_yen[0].astype(float) * 10000
        
        # æå–æ™®é€šæ•°å€¼
        normal_yen = df['unit_price'].str.extract(r'(\d+(?:,\d+)?)(?=å††|$)').dropna()
        if not normal_yen.empty:
            df.loc[normal_yen.index, 'unit_price_numeric'] = normal_yen[0].str.replace(',', '').astype(float)

    if 'received_at' in df.columns:
        df['received_at'] = df['received_at'].astype(str)

    export_columns = [
        "message_id","received_at", "subject", "sender_email", "project_description",
        "required_skills", "optional_skills", "location", "unit_price" 
    ]
    existing_columns = [col for col in export_columns if col in df.columns]
    df_export = df[existing_columns].fillna('')

    # è‹±èªâ†’æ—¥æœ¬èªã®åˆ—åãƒãƒƒãƒ”ãƒ³ã‚°
    column_mapping = {
        "message_id": "ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ID",
        "received_at": "å—ä¿¡æ—¥æ™‚",
        "subject": "ä»¶å",
        "sender_email": "é€ä¿¡è€…ãƒ¡ãƒ¼ãƒ«",
        "project_description": "æ¡ˆä»¶å†…å®¹",
        "required_skills": "å¿…é ˆã‚¹ã‚­ãƒ«",
        "optional_skills": "å°šå¯ã‚¹ã‚­ãƒ«",
        "location": "å‹¤å‹™åœ°",
        "unit_price": "å˜ä¾¡"
    }

    # åˆ—ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’æ—¥æœ¬èªã«å¤‰æ›ã—ã¦ãƒ‡ãƒ¼ã‚¿æ•´å½¢
    header = [column_mapping.get(col, col) for col in df_export.columns]
    data = [header] + [
    [sanitize_text(cell) for cell in row]
    for row in df_export.values.tolist()
]

    body = {'values': data}

    try:
        result = service.spreadsheets().values().update(
            spreadsheetId=spreadsheet_id,
            range=sheet_name,
            valueInputOption='RAW',
            body=body
        ).execute()
        print(f"âœ… ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†ã€åˆè¨ˆ {len(df_export)} è¡Œã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ã¾ã—ãŸã€‚")
    except Exception as e:
        print(f"âŒ ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã¸ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")


# -------------------------
# ãƒ˜ãƒƒãƒ€ãƒ¼ãƒ»æœ¬æ–‡ æŠ½å‡ºãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# -------------------------
def extract_headers(msg, name):
    headers = msg.get('payload', {}).get('headers', [])
    for h in headers:
        if h.get('name', '').lower() == name.lower():
            return h.get('value', '')
    return ''


def extract_body(msg) -> str:
    payload = msg.get('payload', {})
    parts = payload.get('parts', [])
    for part in parts:
        if part.get('mimeType') == 'text/plain':
            data = part.get('body', {}).get('data', '')
            if data:
                return base64.urlsafe_b64decode(data).decode('utf-8')

    if 'body' in payload and 'data' in payload['body']:
        return base64.urlsafe_b64decode(payload['body']['data']).decode('utf-8')

    return msg.get('snippet', '')


def format_datetime(gmail_date):
    try:
        gmail_date = re.sub(r'\s*\(.*\)$', '', gmail_date)
        return datetime.strptime(gmail_date, '%a, %d %b %Y %H:%M:%S %z').strftime('%Y-%m-%d %H:%M:%S')
    except Exception as e:
        logging.error(f"æ—¥ä»˜ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")
        return datetime.now().strftime('%Y-%m-%d %H:%M:%S')


# -------------------------
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# -------------------------
def main():
    logging.info("ğŸ” Gmailã‹ã‚‰ãƒ¡ãƒ¼ãƒ«ã‚’å–å¾—ä¸­...")
    email_data_list = []

    try:
        service = get_gmail_service()
        emails = fetch_ses_emails(service)

        if not emails:
            logging.warning("ğŸ“­ ä»Šæ—¥ã®ãƒ¡ãƒ¼ãƒ«ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return []

        logging.info(f"\nğŸ“© {len(emails)} ä»¶ã®è©²å½“ã™ã‚‹ãƒ¡ãƒ¼ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")

        parser = GeminiParser()

        for i, email in enumerate(emails, 1):
            logging.info(f"\n--- ãƒ¡ãƒ¼ãƒ« {i}/{len(emails)} ã‚’å‡¦ç†ä¸­ ---")

            subject = extract_headers(email, 'Subject')
            sender = extract_headers(email, 'From')
            date = format_datetime(extract_headers(email, 'Date'))
            body_text = extract_body(email)

            logging.info(f"ä»¶å: {subject}")
            logging.info(f"é€ä¿¡è€…: {sender}")
            logging.info(f"æ—¥ä»˜: {date}")

            if not body_text.strip():
                logging.warning("âš ï¸ æœ¬æ–‡ãŒç©ºã§ã™ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
                continue

            try:
                parsed = parser.parse_email(body_text)
                logging.info("è§£æçµæœ:")
                logging.info(json.dumps(parsed, indent=2, ensure_ascii=False))

                email_data = {
                    'received_at': date,
                    'subject': subject,
                    'sender_email': sender,
                    'project_description': parsed.get('æ¡ˆä»¶å†…å®¹', ''),
                    'required_skills': parsed.get('å¿…é ˆã‚¹ã‚­ãƒ«', []),
                    'optional_skills': parsed.get('å°šå¯ã‚¹ã‚­ãƒ«', []),
                    "location": parsed.get("å‹¤å‹™åœ°", ""),
                    "unit_price": parsed.get("å˜ä¾¡", ""),
                    'message_id': email.get('id')
                }

                email_data_list.append(email_data)

            except Exception as e:
                logging.error(f"âŒ ãƒ¡ãƒ¼ãƒ«å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")

    except Exception as e:
        logging.error(f"ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚°ãƒ©ãƒ å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")

    return email_data_list


# -------------------------
# å®Ÿè¡Œãƒ–ãƒ­ãƒƒã‚¯
# -------------------------
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")

    SPREADSHEET_ID = os.getenv("SPREADSHEET_ID")
    email_data_list = main()

    if email_data_list:
       
        export_to_sheet(email_data_list, SPREADSHEET_ID,sheet_name="ã‚·ãƒ¼ãƒˆ1")
    else:
        print("ğŸ“­ æœ‰åŠ¹ãªãƒ¡ãƒ¼ãƒ«ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
