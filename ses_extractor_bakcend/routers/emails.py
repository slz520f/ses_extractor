from fastapi import APIRouter, HTTPException
from fastapi.responses import JSONResponse


import os
import logging
from datetime import datetime, timedelta, timezone
from dotenv import load_dotenv
from routers.auth import token_store
from supabase import create_client, Client
from typing import Set

from utils.emails import fetch_ses_emails, refresh_access_token
from utils.gemini_and_db import parse_emails_with_gemini, send_to_api



# åˆå§‹åŒ–
router = APIRouter()

load_dotenv()

# é…ç½®
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_API_KEY = os.getenv("SUPABASE_KEY")
GEMINI_API_KEY = os.getenv("GOOGLE_API_KEY")
GMAIL_API_URL = "https://gmail.googleapis.com/gmail/v1/users/me/messages"

supabase: Client = create_client(
    os.getenv("SUPABASE_URL"),
    os.getenv("SUPABASE_KEY")
)

# æ—¥å¿—é…ç½®
logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)


def get_parsed_message_ids() -> Set[str]:
    from supabase import create_client
    supabase = create_client(SUPABASE_URL, SUPABASE_API_KEY)
    result = supabase.table("ses_projects").select("message_id").execute()
    ids = set(item["message_id"] for item in result.data if item.get("message_id"))
    return ids


# è·å– SES é‚®ä»¶æ¥å£
@router.get("/fetch_emails")
async def fetch_emails():
    access_token = token_store.get("access_token")
    refresh_token = token_store.get("refresh_token")
    logger.info(f"Access token: {access_token}")
    if not access_token:
        raise HTTPException(status_code=401, detail="æœªç™»å½•æˆ–Tokenå·²å¤±æ•ˆ")

    try:
        emails = fetch_ses_emails(access_token)
        return {"emails": emails}

    except Exception as e:
        # å¤„ç†tokenè¿‡æœŸ
        if "invalid_grant" in str(e) or "Invalid Credentials" in str(e) or "401" in str(e):
            logger.warning("Access token å·²è¿‡æœŸï¼Œå°è¯•åˆ·æ–°...")
            if not refresh_token:
                raise HTTPException(status_code=401, detail="Refresh token ä¸å­˜åœ¨ï¼Œæ— æ³•åˆ·æ–°ã€‚")
            
            try:
                new_access_token = refresh_access_token(refresh_token)
                token_store["access_token"] = new_access_token
                logger.info("âœ… Access token åˆ·æ–°æˆåŠŸï¼Œé‡æ–°å°è¯•fetché‚®ä»¶...")
                
                # åˆ·æ–°åé‡æ–°å°è¯•
                emails = fetch_ses_emails(new_access_token)
                return {"emails": emails}
            
            except Exception as refresh_error:
                logger.error(f"Tokenåˆ·æ–°å¤±è´¥: {str(refresh_error)}")
                raise HTTPException(status_code=401, detail="Tokenåˆ·æ–°å¤±è´¥ï¼Œè¯·é‡æ–°ç™»å½•ã€‚")
        else:
            logger.error(f"æœªçŸ¥é”™è¯¯: {str(e)}")
            raise HTTPException(status_code=500, detail=f"å†…éƒ¨é”™è¯¯: {str(e)}")
    


# è§£ææ‰€æœ‰é‚®ä»¶å¹¶ä¿å­˜åˆ°æ•°æ®åº“
@router.post("/parse_and_save_all_emails")
async def parse_and_save_all_emails():
    access_token = token_store.get("access_token")
    if not access_token:
        raise HTTPException(status_code=401, detail="æœªç™»å½•æˆ–Tokenå·²å¤±æ•ˆ")

    # è·å–é‚®ä»¶
    ses_emails = fetch_ses_emails(access_token)
    total_fetched = len(ses_emails)
    logging.info(f"ğŸ“¥ æŠ“å–åˆ°çš„é‚®ä»¶æ€»æ•°: {total_fetched}")

    # è·å–å·²ç»è§£æè¿‡çš„ message_id
    parsed_ids = get_parsed_message_ids()
    logging.info(f"ğŸ“‚ æ•°æ®åº“ä¸­å·²å­˜åœ¨çš„è§£æè®°å½•æ•°: {len(parsed_ids)}")

    # è¿‡æ»¤æ‰å·²è§£æçš„é‚®ä»¶
    unparsed_emails = [email for email in ses_emails if email.get("id") not in parsed_ids]
    logging.info(f"ğŸ§¹ è¿‡æ»¤åæœªè§£æé‚®ä»¶æ•°: {len(unparsed_emails)}")
    logging.info(f"â™»ï¸ é‡å¤ï¼ˆå·²è§£æï¼‰é‚®ä»¶æ•°: {total_fetched - len(unparsed_emails)}")

    if not unparsed_emails:
        logging.info("âœ… æ²¡æœ‰æ–°é‚®ä»¶éœ€è¦è§£æã€‚")
        return JSONResponse(content={
            "status": "success", 
            "processed_count": 0, 
            "message": "æ²¡æœ‰æ–°é‚®ä»¶éœ€è¦è§£æ",
            "logs": {
                "total_fetched": total_fetched,
                "parsed_count": len(parsed_ids),
                "unparsed_count": 0,
                "duplicate_count": total_fetched,
                "processed_count": 0,
                "message": "æ²¡æœ‰æ–°é‚®ä»¶éœ€è¦è§£æ"
            }
        })

    # ä½¿ç”¨Geminiè§£æ
    email_data_list = parse_emails_with_gemini(unparsed_emails)

    # å‘é€åˆ°APIæˆ–ä¿å­˜æ•°æ®åº“
    send_to_api(email_data_list)

    logging.info(f"âœ… å®é™…è§£æå¹¶å†™å…¥çš„é‚®ä»¶æ•°: {len(email_data_list)}")

    return JSONResponse(content={
        "status": "success",
        "processed_count": len(email_data_list),
        "logs": {
            "total_fetched": total_fetched,
            "parsed_count": len(parsed_ids),
            "unparsed_count": len(unparsed_emails),
            "duplicate_count": total_fetched - len(unparsed_emails),
            "processed_count": len(email_data_list),
            "message": "è§£ææˆåŠŸ"
        }
    })





@router.get("/recent")
async def get_recent_emails():
    """
    è·å–è¿‘5å¤©çš„é‚®ä»¶æ•°æ®ï¼ˆä¿®å¤ç‰ˆï¼‰
    """
    try:
        # è·å–å½“å‰æ—¶é—´ï¼ˆå¸¦æ—¶åŒºä¿¡æ¯ï¼‰
        now = datetime.now(timezone.utc)
        five_days_ago = now - timedelta(days=5)

        logger.info(f"æŸ¥è¯¢æ—¶é—´èŒƒå›´: {five_days_ago.isoformat()} è‡³ {now.isoformat()}")
        
       

       # æŸ¥è¯¢æ•°æ®åº“
        response = supabase.table('ses_projects') \
            .select('*') \
            .gte('received_at', five_days_ago.isoformat()) \
            .order('received_at', desc=True) \
            .execute()

        # æ£€æŸ¥é”™è¯¯
        if hasattr(response, 'error') and response.error:
            logger.error(f"SupabaseæŸ¥è¯¢é”™è¯¯: {response.error}")
            raise HTTPException(status_code=500, detail="æ•°æ®åº“æŸ¥è¯¢å¤±è´¥")

        logger.info(f"æŸ¥è¯¢åˆ° {len(response.data)} æ¡è®°å½•")
        return {"emails": response.data}

    except Exception as e:
        logger.error(f"è·å–è¿‘æœŸé‚®ä»¶å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail="è·å–è¿‘æœŸé‚®ä»¶å¤±è´¥")
    


@router.get("/test")
async def test_endpoint():
    return {"message": "æµ‹è¯•ç«¯ç‚¹æ­£å¸¸å·¥ä½œ"}