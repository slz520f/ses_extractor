from fastapi import APIRouter, HTTPException
from fastapi.responses import JSONResponse
from fastapi import FastAPI
import requests
import os
import re
import logging
import google.generativeai as genai
import json
import base64
from typing import Dict, Any
from datetime import datetime, timedelta, timezone
from dotenv import load_dotenv
from routers.auth import token_store
from supabase import create_client  # å¯¼å…¥ supabase å®¢æˆ·ç«¯
from typing import List, Dict, Any  # Ensure List is imported
from googleapiclient.discovery import build
from google.oauth2.credentials import Credentials
from supabase import create_client, Client

from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.interval import IntervalTrigger
import atexit
from contextlib import asynccontextmanager


# åˆå§‹åŒ–
router = APIRouter()
app = FastAPI()
load_dotenv()

# é…ç½®
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_API_KEY = os.getenv("SUPABASE_KEY")
GEMINI_API_KEY = os.getenv("GOOGLE_API_KEY")
GMAIL_API_URL = "https://gmail.googleapis.com/gmail/v1/users/me/messages"

supabase: Client = create_client(
    os.getenv("SUPABASE_URL"),
    os.getenv("SUPABASE_KEY")
)

# æ—¥å¿—é…ç½®
logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)

# åˆå§‹åŒ–å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨
scheduler = BackgroundScheduler(daemon=True)
scheduler.start()
atexit.register(lambda: scheduler.shutdown())  # ç¡®ä¿åº”ç”¨é€€å‡ºæ—¶å…³é—­è°ƒåº¦å™¨

def scheduled_email_fetching(access_token: str = None):
    """
    æ‰§è¡Œé‚®ä»¶è·å–ã€è§£æå’Œä¿å­˜çš„å®Œæ•´æµç¨‹
    """
    if not access_token:
        access_token = token_store.get("access_token")
        if not access_token:
            logger.warning("æœªç™»å½•æˆ–Tokenå·²å¤±æ•ˆï¼Œè·³è¿‡é‚®ä»¶è·å–")
            raise ValueError("æœªç™»å½•æˆ–Tokenå·²å¤±æ•ˆ")

    try:
        logger.info("â° å¼€å§‹è·å–é‚®ä»¶...")
        
        # 1. è·å–é‚®ä»¶
        ses_emails = fetch_ses_emails(access_token)
        logger.info(f"ğŸ“¨ è·å–åˆ° {len(ses_emails)} å°æ–°é‚®ä»¶")
        
        if not ses_emails:
            logger.info("æ²¡æœ‰æ–°é‚®ä»¶éœ€è¦å¤„ç†")
            return {"status": "success", "message": "æ²¡æœ‰æ–°é‚®ä»¶"}
        
        # 2. ä½¿ç”¨Geminiè§£æ
        email_data_list = parse_emails_with_gemini(ses_emails)
        logger.info(f"ğŸ” æˆåŠŸè§£æ {len(email_data_list)} å°é‚®ä»¶")
        
        # 3. ä¿å­˜åˆ°æ•°æ®åº“
        send_to_api(email_data_list)
        logger.info("ğŸ’¾ é‚®ä»¶æ•°æ®å·²ä¿å­˜åˆ°æ•°æ®åº“")
        
        return {"status": "success", "processed": len(email_data_list)}
        
    except Exception as e:
        logger.error(f"é‚®ä»¶å¤„ç†å¤±è´¥: {str(e)}")
        raise

@asynccontextmanager
async def lifespan(app: FastAPI):
    if not scheduler.get_jobs():
        scheduler.add_job(
            func=lambda: scheduled_email_fetching(token_store.get("access_token")),
            trigger=IntervalTrigger(hours=1),
            id="email_fetching_job"
        )
        logger.info("âœ… å®šæ—¶é‚®ä»¶æœåŠ¡å·²å¯åŠ¨")
    yield
    scheduler.shutdown()

app = FastAPI(lifespan=lifespan)

@router.post("/trigger-email-fetch")
async def manual_trigger():
    """
    æ‰‹åŠ¨è§¦å‘é‚®ä»¶è·å–
    """
    try:
        access_token = token_store.get("access_token")
        if not access_token:
            raise HTTPException(status_code=401, detail="æœªç™»å½•æˆ–Tokenå·²å¤±æ•ˆ")
        
        result = scheduled_email_fetching(access_token)
        return JSONResponse(content=result)
        
    except Exception as e:
        logger.exception("æ‰‹åŠ¨è§¦å‘å¤±è´¥")
        raise HTTPException(status_code=500, detail=str(e))    

def get_recent_emails():
    """
    è·å–è¿‘14å¤©çš„é‚®ä»¶æ•°æ®
    :return: è¿”å›é‚®ä»¶åˆ—è¡¨
    """
    # è·å–å½“å‰æ—¶é—´å’Œ5å¤©å‰çš„æ—¶é—´
    five_days_ago = (datetime.utcnow() - timedelta(days=14)).isoformat()

    # æŸ¥è¯¢ ses_projects è¡¨ï¼Œç­›é€‰å‡ºæ¥æ”¶æ—¶é—´åœ¨è¿‘5å¤©å†…çš„è®°å½•
    response = supabase.table('ses_projects') \
        .select('*') \
        .filter('received_at', 'gte', five_days_ago) \
        .execute()

    if response.status_code == 200:
        return response.data  # è¿”å›æŸ¥è¯¢åˆ°çš„é‚®ä»¶æ•°æ®
    else:
        return {"error": "æŸ¥è¯¢å¤±è´¥", "message": response.message}



def refresh_access_token(refresh_token: str):
    """ä½¿ç”¨ refresh_token è·å–æ–°çš„ access_token"""
    token_url = "https://oauth2.googleapis.com/token"
    client_id = os.getenv("GOOGLE_CLIENT_ID")
    client_secret = os.getenv("GOOGLE_CLIENT_SECRET")
    
    if not client_id or not client_secret:
        raise ValueError("ç¼ºå°‘ GOOGLE_CLIENT_ID æˆ– GOOGLE_CLIENT_SECRET ç¯å¢ƒå˜é‡ã€‚")
    
    payload = {
        "client_id": client_id,
        "client_secret": client_secret,
        "refresh_token": refresh_token,
        "grant_type": "refresh_token",
    }
    response = requests.post(token_url, data=payload)
    
    if response.status_code == 200:
        tokens = response.json()
        return tokens.get("access_token")
    else:
        raise Exception(f"Tokenåˆ·æ–°å¤±è´¥: {response.text}")


def extract_headers(msg, name):
    """ãƒ¡ãƒ¼ãƒ«ãƒ˜ãƒƒãƒ€ã‹ã‚‰ç‰¹å®šã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æŠ½å‡º"""
    headers = msg.get('payload', {}).get('headers', [])
    for h in headers:
        if h.get('name', '').lower() == name.lower():
            return h.get('value', '')
    return ''

def extract_body(msg) -> str:
    """Gmailãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰ãƒ—ãƒ¬ãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæœ¬æ–‡ã‚’æŠ½å‡º"""
    payload = msg.get('payload', {})
    
    # partsã‹ã‚‰æŠ½å‡ºã‚’è©¦ã¿ã‚‹
    parts = payload.get('parts', [])
    for part in parts:  
        if part.get('mimeType') == 'text/plain':
            data = part.get('body', {}).get('data', '')
            if data:
                return base64.urlsafe_b64decode(data).decode('utf-8')
    
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šç›´æ¥body.dataã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
    if 'body' in payload and 'data' in payload['body']:
        return base64.urlsafe_b64decode(payload['body']['data']).decode('utf-8')
    
    return msg.get('snippet', '')

def format_datetime(gmail_date):
    """æ—¥ä»˜ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    try:
        # JSTãªã©ã®ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³åã‚’å–ã‚Šé™¤ã
        gmail_date = re.sub(r'\s*\(.*\)$', '', gmail_date)
        return datetime.strptime(gmail_date, '%a, %d %b %Y %H:%M:%S %z').strftime('%Y-%m-%d %H:%M:%S')
    except Exception as e:
        logging.error(f"æ—¥ä»˜ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")
        return datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    

def get_today_date_query():
    """å½“æ—¥é™å®šã®æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ä½œæˆã™ã‚‹é–¢æ•°"""
    today = datetime.now()
    tomorrow = today + timedelta(days=1)

    after_str = today.strftime('%Y/%m/%d')
    before_str = tomorrow.strftime('%Y/%m/%d')

    return f'after:{after_str} before:{before_str}'    


# æ–°å¢å‡½æ•°ï¼šè·å–æˆ–æ›´æ–°æœ€åè·å–æ—¶é—´
def get_last_fetch_time():
    result = supabase.table('email_fetch_status').select('*').order('last_fetch_time', desc=True).limit(1).execute()
    if result.data:
        return result.data[0]['last_fetch_time']
    return None

def update_last_fetch_time(new_time, last_message_id=None):
    supabase.table('email_fetch_status').insert({
        'last_fetch_time': new_time,
        'last_message_id': last_message_id
    }).execute()    



# ä¿®æ”¹fetch_ses_emailså‡½æ•°
def fetch_ses_emails(access_token: str, progress_bar=None, query=" has:nouserlabels"):
    """å¢é‡è·å–SESæ¡ˆä»¶é‚®ä»¶"""
    last_fetch = get_last_fetch_time()
    date_query = ""
    
    if last_fetch:
        # åªè·å–ä¸Šæ¬¡è·å–æ—¶é—´ä¹‹åçš„é‚®ä»¶
        last_fetch_str = last_fetch.strftime('%Y/%m/%d %H:%M:%S')
        date_query = f" after:{last_fetch_str}"
    else:
        # ç¬¬ä¸€æ¬¡è·å–ï¼Œåªè·å–å½“å¤©çš„
        date_query = get_today_date_query()
    
    full_query = f"{query}{date_query}"
    print(f"æ‰§è¡ŒæŸ¥è¯¢: {full_query}")
    
    credentials = Credentials(token=access_token)
    service = build('gmail', 'v1', credentials=credentials)
    
    # åˆ†æ‰¹è·å–ï¼ˆæ¯æ¬¡200å°ï¼‰
    messages = []
    page_token = None
    batch_size = 200  # æ¯æ¬¡è·å–200å°
    
    while True:
        results = service.users().messages().list(
            userId='me',
            q=full_query,
            maxResults=batch_size,
            pageToken=page_token
        ).execute()
        
        batch = results.get('messages', [])
        messages.extend(batch)
        
        # æ›´æ–°æœ€åè·å–æ—¶é—´ï¼ˆä½¿ç”¨è¿™æ‰¹é‚®ä»¶ä¸­æœ€æ–°çš„æ—¶é—´ï¼‰
        if batch:
            newest_msg = service.users().messages().get(
                userId='me',
                id=batch[0]['id'],
                format='metadata',
                metadataHeaders=['Date']
            ).execute()
            
            msg_date = extract_headers(newest_msg, 'Date')
            if msg_date:
                update_last_fetch_time(format_datetime(msg_date), batch[0]['id'])
        
        page_token = results.get('nextPageToken')
        if not page_token or len(messages) >= batch_size:
            break
    
    # è¿‡æ»¤æ‰æœ‰é™„ä»¶çš„é‚®ä»¶
    ses_emails = []
    for msg_meta in messages[:batch_size]:  # åªå¤„ç†å½“å‰æ‰¹æ¬¡çš„200å°
        msg = service.users().messages().get(
            userId='me',
            id=msg_meta['id'],
            format='full'
        ).execute()
        
        parts = msg.get('payload', {}).get('parts', [])
        has_attachment = any(
            part.get('filename') and part['filename'] != ''
            for part in parts
        )
        
        if not has_attachment:
            ses_emails.append(msg)
    
    return ses_emails

#åŸæ¥çš„èƒ½ç”¨çš„æ‰¾é‚®ä»¶
# def fetch_ses_emails(access_token: str, progress_bar=None,query="(æ¡ˆä»¶ï¼‰ has:nouserlabels "):
#     """å½“æ—¥å—ä¿¡ã—ãŸSESæ¡ˆä»¶ãƒ¡ãƒ¼ãƒ«ï¼ˆæ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ãªã—ï¼‰ã‚’å–å¾—"""
#     date_query = get_today_date_query()
#     full_query = f" {query}{date_query}"

#     # ä½¿ç”¨ access_token åˆ›å»ºå‡­è¯å¯¹è±¡
#     credentials = Credentials(token=access_token)

#     # åˆå§‹åŒ– Google API å®¢æˆ·ç«¯
#     service = build('gmail', 'v1', credentials=credentials)

#     # ç¡®ä¿ service æ˜¯ä¸€ä¸ªæ­£ç¡®çš„ API å®¢æˆ·ç«¯å¯¹è±¡
#     print(f"Service type: {type(service)}")  # ç¡®ä¿è¿™é‡Œè¾“å‡ºçš„æ˜¯ <class 'googleapiclient.discovery.Resource'>
    
#     print(f"å®Ÿè¡Œã‚¯ã‚¨ãƒª: {full_query}")
    
#     # ãƒ¡ãƒ¼ãƒ«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ä¸€è¦§ã‚’å–å¾—
#     results = service.users().messages().list(
#         userId='me', 
#         q=full_query, 
#         maxResults=2000  # æœ€å¤§2000ä»¶
#     ).execute()
    
#     messages = results.get('messages', [])
#     ses_emails = []
    
#     for msg_meta in messages:
#         # å„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®è©³ç´°ã‚’å–å¾—
#         msg = service.users().messages().get(
#             userId='me', 
#             id=msg_meta['id'], 
#             format='full'
#         ).execute()
        
#         # æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
#         parts = msg.get('payload', {}).get('parts', [])
#         has_attachment = any(
#             part.get('filename') and part['filename'] != '' 
#             for part in parts
#         )
        
#         # æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ãªã—ã®ãƒ¡ãƒ¼ãƒ«ã®ã¿è¿½åŠ 
#         if not has_attachment:
#             ses_emails.append(msg)
    
#     return ses_emails

class GeminiParser:
    def __init__(self, model_name: str = "gemini-1.5-flash-latest"):
        """
        Google Generative AI (Gemini) ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
        
        Args:
            model_name (str): ãƒ¢ãƒ‡ãƒ«å
        """
        logger.info(f"Using Gemini model: {model_name}")
        self.model = genai.GenerativeModel(model_name)

    def _construct_prompt(self, text: str) -> str:
        """
        ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ§‹ç¯‰ï¼ˆæ—¥æœ¬èªã§æ˜ç¢ºãªæŒ‡ç¤ºï¼‰
        """
        return f"""ä»¥ä¸‹ã¯SESæ¡ˆä»¶ãƒ¡ãƒ¼ãƒ«ã®æœ¬æ–‡ã§ã™ã€‚ä»¥ä¸‹ã®é …ç›®ã‚’æ—¥æœ¬èªã§æŠ½å‡ºã—ã€JSONå½¢å¼ã§è¿”ã—ã¦ãã ã•ã„ã€‚

### æŠ½å‡ºé …ç›®:
- æ¡ˆä»¶å†…å®¹ï¼ˆä»•äº‹ã®è©³ç´°ï¼‰
- å¿…é ˆã‚¹ã‚­ãƒ«ï¼ˆå¿…é ˆæŠ€è¡“ãƒ»è³‡æ ¼ï¼‰
- å°šå¯ã‚¹ã‚­ãƒ«ï¼ˆã‚ã‚Œã°è‰¯ã„æŠ€è¡“ï¼‰
- å‹¤å‹™åœ°ï¼ˆéƒ½é“åºœçœŒã¾ãŸã¯å¸‚åŒºç”ºæ‘ï¼‰
- å˜ä¾¡ï¼ˆ"Â¥"ã‚„"å††"ã‚’å«ã‚€æ–‡å­—åˆ—ï¼‰

### å‡ºåŠ›å½¢å¼:
{{
  "æ¡ˆä»¶å†…å®¹": "...",
  "å¿…é ˆã‚¹ã‚­ãƒ«": ["...", "..."],
  "å°šå¯ã‚¹ã‚­ãƒ«": ["...", "..."],
  "å‹¤å‹™åœ°": "...",
  "å˜ä¾¡": "..."
}}

### ãƒ¡ãƒ¼ãƒ«æœ¬æ–‡:
{text}

### JSONå‡ºåŠ›:"""

    def _parse_output(self, output: str) -> Dict[str, Any]:
        """
        ãƒ¢ãƒ‡ãƒ«ã®ç”Ÿå‡ºåŠ›ã‹ã‚‰JSONéƒ¨åˆ†ã‚’æŠ½å‡º
        """
        json_match = re.search(r'\{[\s\S]*\}', output)
        if not json_match:
            raise ValueError("å‡ºåŠ›ã«JSONãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        json_str = json_match.group(0)
        
        try:
            parsed = json.loads(json_str)
            required_fields = ["æ¡ˆä»¶å†…å®¹", "å¿…é ˆã‚¹ã‚­ãƒ«", "å‹¤å‹™åœ°", "å˜ä¾¡"]
            for field in required_fields:
                if field not in parsed:
                    raise ValueError(f"å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ '{field}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                    
            if isinstance(parsed["å¿…é ˆã‚¹ã‚­ãƒ«"], str):
                parsed["å¿…é ˆã‚¹ã‚­ãƒ«"] = [s.strip() for s in parsed["å¿…é ˆã‚¹ã‚­ãƒ«"].split(",") if s.strip()]
            if isinstance(parsed.get("å°šå¯ã‚¹ã‚­ãƒ«", []), str):
                parsed["å°šå¯ã‚¹ã‚­ãƒ«"] = [s.strip() for s in parsed["å°šå¯ã‚¹ã‚­ãƒ«"].split(",") if s.strip()]
                
            return parsed
            
        except json.JSONDecodeError as e:
            logger.error(f"JSONè§£æã‚¨ãƒ©ãƒ¼: {e}\nå‡ºåŠ›å†…å®¹: {output}")
            raise

    def parse_email(self, text: str) -> Dict[str, Any]:
        """
        ãƒ¡ãƒ¼ãƒ«æœ¬æ–‡ã‚’è§£æã—ã¦æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™
        
        Args:
            text (str): ãƒ¡ãƒ¼ãƒ«æœ¬æ–‡ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            dict: è§£æçµæœï¼ˆã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ãŒå…¥ã‚‹ï¼‰
        """


        

        prompt = self._construct_prompt(text)

        try:
            response = self.model.generate_content(prompt)
            output = response.text.strip()
            logger.debug(f"ãƒ¢ãƒ‡ãƒ«ç”Ÿå‡ºåŠ›:\n{output}")
            return self._parse_output(output)
            
        except Exception as e:
            logger.error(f"äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return {
            "æ¡ˆä»¶å†…å®¹": "",
            "å¿…é ˆã‚¹ã‚­ãƒ«": [],
            "å°šå¯ã‚¹ã‚­ãƒ«": [],
            "å‹¤å‹™åœ°": "",
            "å˜ä¾¡": ""
        }


def parse_emails_with_gemini(emails: List[dict], progress_callback=None, api_key=None) -> List[Dict]:
    import google.generativeai as genai

    # APIã‚­ãƒ¼ã®æ§‹æˆï¼ˆå¼•æ•°å„ªå…ˆã€ãªã‘ã‚Œã°ç’°å¢ƒå¤‰æ•°ï¼‰
    if api_key:
        genai.configure(api_key=api_key)
        logger.info("âœ… æ˜ç¤ºã•ã‚ŒãŸ API ã‚­ãƒ¼ã§ Gemini ã‚’æ§‹æˆã—ã¾ã—ãŸã€‚")
    else:
        env_key = os.getenv("GOOGLE_API_KEY")
        if env_key:
            genai.configure(api_key=env_key)
            logger.info("âœ… ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ API ã‚­ãƒ¼ã‚’èª­ã¿å–ã‚Šã¾ã—ãŸã€‚")
        else:
            logger.error("âŒ Gemini APIã‚­ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            raise ValueError("Gemini APIã‚­ãƒ¼ãŒå¿…è¦ã§ã™ã€‚")

    parser = GeminiParser()  # ãƒ‘ãƒ¼ã‚µãƒ¼ã«ã‚‚æ¸¡ã™
    email_data_list = []

    for i, email in enumerate(emails, 1):
        if progress_callback:
            progress_callback(i / len(emails))

        logging.info(f"\n--- ãƒ¡ãƒ¼ãƒ« {i}/{len(emails)} ã‚’å‡¦ç†ä¸­ ---")
        
        subject = extract_headers(email, 'Subject')
        sender = extract_headers(email, 'From')
        date = format_datetime(extract_headers(email, 'Date'))
        body_text = extract_body(email)

        logging.info(f"ä»¶å: {subject}")
        logging.info(f"é€ä¿¡è€…: {sender}")
        logging.info(f"æ—¥ä»˜: {date}")

        if not body_text.strip():
            logging.warning("âš ï¸ æœ¬æ–‡ãŒç©ºã§ã™ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            continue

        try:
            parsed = parser.parse_email(body_text)
            logging.info("è§£æçµæœ:")
            logging.info(json.dumps(parsed, indent=2, ensure_ascii=False))

            email_data = {
                'received_at': date,
                'subject': subject,
                'sender_email': sender,
                'project_description': parsed.get('æ¡ˆä»¶å†…å®¹', ''),
                'required_skills': parsed.get('å¿…é ˆã‚¹ã‚­ãƒ«', []),
                'optional_skills': parsed.get('å°šå¯ã‚¹ã‚­ãƒ«', []),
                "location": parsed.get("å‹¤å‹™åœ°", ""),
                "unit_price": parsed.get("å˜ä¾¡", ""),
                'message_id': email.get('id')
            }

            email_data_list.append(email_data)

        except Exception as e:
            logging.error(f"âŒ ãƒ¡ãƒ¼ãƒ«ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")

    return email_data_list

def send_to_api(email_data_list):
    # åˆå§‹åŒ–Supabaseå®¢æˆ·ç«¯
    supabase = create_client(SUPABASE_URL, SUPABASE_API_KEY)
    success_count = 0
    error_count = 0
    
    for email_data in email_data_list:
        try:
            # å¼ºåŒ–æ•°æ®é¢„å¤„ç†
            data = {
                "message_id": email_data['message_id'],
                "received_at": datetime.strptime(email_data['received_at'], "%Y-%m-%d %H:%M:%S").isoformat(),
                "subject": str(email_data.get('subject', '')),
                "sender_email": str(email_data.get('sender_email', '')),
                "project_description": list_to_str(email_data.get('project_description', []))or "æ— æ¡ˆä»¶æè¿°",  # é»˜è®¤å€¼å¤„ç†
                "required_skills": list_to_str(email_data.get('required_skills', [])),
                "optional_skills": list_to_str(email_data.get('optional_skills', [])),
                "location": list_to_str(email_data.get('location', [])),
                "unit_price": list_to_str(email_data.get('unit_price', []), max_length=500)  # é˜²æ­¢è¶…é•¿
            }

            # åœ¨æ’å…¥å‰è¿›è¡Œæ£€æŸ¥å’Œé»˜è®¤å€¼å¡«å……
            if not data["project_description"]:
                data["project_description"] = "æ— æ¡ˆä»¶æè¿°"  # é»˜è®¤å€¼
                # æ£€æŸ¥message_idæ˜¯å¦å·²å­˜åœ¨äºæ•°æ®åº“ä¸­
            existing_data = supabase.table('ses_projects').select('message_id').eq('message_id', data['message_id']).execute()

            if existing_data.data and len(existing_data.data) > 0:
                # å¦‚æœå·²ç»å­˜åœ¨ï¼Œåˆ™è·³è¿‡æ’å…¥
                logger.info(f"âœ… æ•°æ®å·²å­˜åœ¨ï¼Œè·³è¿‡æ’å…¥: {data['message_id']}")
                continue  # è·³è¿‡æœ¬æ¬¡å¾ªç¯

            # æ’å…¥æ•°æ®åˆ° Supabase
            response = supabase.table('ses_projects').insert(data).execute()


            
            # æ£€æŸ¥æ˜¯å¦æ’å…¥æˆåŠŸ
            if response.data and len(response.data) > 0:
                success_count += 1
                logger.info(f"âœ… æ•°æ®æ’å…¥æˆåŠŸ: {email_data['message_id']}")
            else:
                error_count += 1
                logger.error(f"âŒ æ•°æ®æ’å…¥å¤±è´¥: {response.error}")
                # è¾“å‡ºæ›´å¤šçš„é”™è¯¯ä¿¡æ¯
                if response.error:
                    logger.error(f"âŒ è¿”å›çš„é”™è¯¯ä¿¡æ¯: {response.error}")
                
        except Exception as e:
            error_count += 1
            logger.error(f"âš ï¸ æ•°æ®åº“æ“ä½œå¼‚å¸¸: {str(e)}")
            # è¾“å‡ºå¼‚å¸¸çš„è¯¦ç»†ä¿¡æ¯
            logger.error(f"âš ï¸ å¼‚å¸¸è¯¦æƒ…: {e}")
    
    logger.info(f"å¤„ç†å®Œæˆ: æˆåŠŸ{success_count}æ¡, å¤±è´¥{error_count}æ¡")
                
  

def list_to_str(value, delimiter=", ", max_length=None):
    """å®‰å…¨å¤„ç†åˆ—è¡¨è½¬å­—ç¬¦ä¸²"""
    if isinstance(value, list):
        joined = delimiter.join(str(item) for item in value)
        return joined[:max_length] if max_length else joined
    return str(value)[:max_length] if max_length else str(value)

# è·å– SES é‚®ä»¶æ¥å£
@router.get("/fetch_emails")
async def fetch_emails():
    access_token = token_store.get("access_token")
    refresh_token = token_store.get("refresh_token")
    logger.info(f"Access token: {access_token}")
    if not access_token:
        raise HTTPException(status_code=401, detail="æœªç™»å½•æˆ–Tokenå·²å¤±æ•ˆ")

    try:
        emails = fetch_ses_emails(access_token)
        return {"emails": emails}

    except Exception as e:
        # å¤„ç†tokenè¿‡æœŸ
        if "invalid_grant" in str(e) or "Invalid Credentials" in str(e) or "401" in str(e):
            logger.warning("Access token å·²è¿‡æœŸï¼Œå°è¯•åˆ·æ–°...")
            if not refresh_token:
                raise HTTPException(status_code=401, detail="Refresh token ä¸å­˜åœ¨ï¼Œæ— æ³•åˆ·æ–°ã€‚")
            
            try:
                new_access_token = refresh_access_token(refresh_token)
                token_store["access_token"] = new_access_token
                logger.info("âœ… Access token åˆ·æ–°æˆåŠŸï¼Œé‡æ–°å°è¯•fetché‚®ä»¶...")
                
                # åˆ·æ–°åé‡æ–°å°è¯•
                emails = fetch_ses_emails(new_access_token)
                return {"emails": emails}
            
            except Exception as refresh_error:
                logger.error(f"Tokenåˆ·æ–°å¤±è´¥: {str(refresh_error)}")
                raise HTTPException(status_code=401, detail="Tokenåˆ·æ–°å¤±è´¥ï¼Œè¯·é‡æ–°ç™»å½•ã€‚")
        else:
            logger.error(f"æœªçŸ¥é”™è¯¯: {str(e)}")
            raise HTTPException(status_code=500, detail=f"å†…éƒ¨é”™è¯¯: {str(e)}")
    


# è§£ææ‰€æœ‰é‚®ä»¶å¹¶ä¿å­˜åˆ°æ•°æ®åº“
@router.post("/parse_and_save_all_emails")
async def parse_and_save_all_emails():
    access_token = token_store.get("access_token")
    if not access_token:
        raise HTTPException(status_code=401, detail="æœªç™»å½•æˆ–Tokenå·²å¤±æ•ˆ")

   # è·å–é‚®ä»¶ï¼ˆç›´æ¥è°ƒç”¨ fetch_ses_emailsï¼‰
    ses_emails = fetch_ses_emails(access_token)

    # ä½¿ç”¨Geminiè§£æ
    email_data_list = parse_emails_with_gemini(ses_emails)

    # å‘é€åˆ°APIæˆ–ä¿å­˜æ•°æ®åº“
    send_to_api(email_data_list)

    return JSONResponse(content={"status": "success", "processed_count": len(email_data_list)})



@router.get("/recent")
async def get_recent_emails():
    """
    è·å–è¿‘5å¤©çš„é‚®ä»¶æ•°æ®ï¼ˆä¿®å¤ç‰ˆï¼‰
    """
    try:
        # è·å–å½“å‰æ—¶é—´ï¼ˆå¸¦æ—¶åŒºä¿¡æ¯ï¼‰
        now = datetime.now(timezone.utc)
        five_days_ago = now - timedelta(days=5)

        logger.info(f"æŸ¥è¯¢æ—¶é—´èŒƒå›´: {five_days_ago.isoformat()} è‡³ {now.isoformat()}")
        
       

       # æŸ¥è¯¢æ•°æ®åº“
        response = supabase.table('ses_projects') \
            .select('*') \
            .gte('received_at', five_days_ago.isoformat()) \
            .order('received_at', desc=True) \
            .execute()

        # æ£€æŸ¥é”™è¯¯
        if hasattr(response, 'error') and response.error:
            logger.error(f"SupabaseæŸ¥è¯¢é”™è¯¯: {response.error}")
            raise HTTPException(status_code=500, detail="æ•°æ®åº“æŸ¥è¯¢å¤±è´¥")

        logger.info(f"æŸ¥è¯¢åˆ° {len(response.data)} æ¡è®°å½•")
        return {"emails": response.data}

    except Exception as e:
        logger.error(f"è·å–è¿‘æœŸé‚®ä»¶å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail="è·å–è¿‘æœŸé‚®ä»¶å¤±è´¥")
    
@router.get("/test")
async def test_endpoint():
    return {"message": "æµ‹è¯•ç«¯ç‚¹æ­£å¸¸å·¥ä½œ"}